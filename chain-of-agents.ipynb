{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install PyMuPDF\n",
    "!pip install -U transformers phidata\n",
    "\n",
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from phi.assistant import Assistant\n",
    "from phi.llm.base import LLM\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import re\n",
    "import fitz\n",
    "import gc\n",
    "import random\n",
    "import httpx\n",
    "from pydantic import Field, PrivateAttr\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "SENTENCE_SPLIT_PATTERN = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def extract_text_with_pymupdf(pdf_path):\n",
    "    references_pattern = re.compile(r'^references?$|^bibliography$')\n",
    "    citation_pattern = re.compile(r'^\\[\\d+\\]|^\\d+\\.|^[A-Z][a-z]+,\\s*[A-Z]\\.|^\\s*[A-Z][a-z]+\\s+[A-Z][a-z]+')\n",
    "    number_pattern = re.compile(r'^\\[\\d+\\]$|^\\d+\\.$')\n",
    "    year_pattern = re.compile(r'^.*?\\(\\d{4}\\)[.,].*?$')\n",
    "    whitespace_pattern = re.compile(r'\\s+')\n",
    "    \n",
    "    full_text = []\n",
    "    in_references = False\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                blocks = page.get_text(\"blocks\")\n",
    "                blocks.sort(key=lambda b: (b[1], b[0]))\n",
    "                \n",
    "                for block in blocks:\n",
    "                    text = block[4].strip() if block[4] else ''\n",
    "                    if not text:\n",
    "                        continue\n",
    "                        \n",
    "                    if references_pattern.match(text.lower()):\n",
    "                        in_references = True\n",
    "                        continue\n",
    "                        \n",
    "                    if in_references and citation_pattern.match(text):\n",
    "                        continue\n",
    "                    \n",
    "                    if number_pattern.match(text):\n",
    "                        continue\n",
    "                        \n",
    "                    if year_pattern.match(text) and len(text.split()) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    text = whitespace_pattern.sub(' ', text)\n",
    "                    if text.isupper() or len(text) <= 3:\n",
    "                        full_text.append(f\"\\n## {text}\\n\")\n",
    "                    else:\n",
    "                        full_text.append(text + \"\\n\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing PDF file: {str(e)}\")\n",
    "    \n",
    "    return \"\\n\".join(full_text)\n",
    "\n",
    "def extract_text(file_path):\n",
    "    if file_path.lower().endswith('.txt'):\n",
    "        return read_text_file(file_path)\n",
    "    elif file_path.lower().endswith('.pdf'):\n",
    "        return extract_text_with_pymupdf(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format. File must be either .pdf or .txt\")\n",
    "\n",
    "@dataclass\n",
    "class TaskConfig:\n",
    "    first_worker_instruction: str\n",
    "    worker_instruction: str\n",
    "    manager_instruction: str\n",
    "    multi_summary_instruction: str\n",
    "\n",
    "class TaskType(Enum):\n",
    "    QA = \"qa\"\n",
    "    SUMMARIZATION = \"summarization\"\n",
    "\n",
    "class TaskFactory:\n",
    "    @staticmethod\n",
    "    def create_task(task_type: TaskType) -> TaskConfig:\n",
    "        if task_type == TaskType.QA:\n",
    "            return TaskConfig(\n",
    "                first_worker_instruction=\"\"\"\n",
    "                [CONTEXT]\n",
    "                {chunk_text}\n",
    "                [QUESTION]\n",
    "                {question}\n",
    "                [TASK]\n",
    "                List several facts from the provided context that might help to answer the question:\n",
    "                - [Fact 1]\n",
    "                - [Fact 2]\n",
    "                ...\n",
    "                Provide a [SUMMARY] by summarizing all the relevant information related to the question.\n",
    "                Keep the existing structure in focus. Do not answer the question.\n",
    "                \"\"\",\n",
    "                worker_instruction=\"\"\"\n",
    "                [SUMMARY]\n",
    "                {previous_summary}\n",
    "                [CONTEXT]\n",
    "                {chunk_text}\n",
    "                [QUESTION]\n",
    "                {question}\n",
    "                [TASK]\n",
    "                List several facts from the provided context that might help to answer the question:\n",
    "                - [Fact 1]\n",
    "                - [Fact 2]\n",
    "                ...\n",
    "                Prioritize all relevant information to the question and then refine the current summary by including the new additional information in [REVISED SUMMARY].\n",
    "                Do not answer the question.\n",
    "                \"\"\",\n",
    "                manager_instruction=\"\"\"\n",
    "                [SUMMARY]\n",
    "                {last_summary}\n",
    "                [QUESTION]\n",
    "                {question}\n",
    "                [TASK]\n",
    "                Using all context available, resolve any contradictions, and provide a comprehensive answer below.\n",
    "                Answer format: <answer>...</answer>\n",
    "                \"\"\",\n",
    "                multi_summary_instruction=\"\"\"\n",
    "                [SUMMARIES]\n",
    "                {last_summary}\n",
    "                [QUESTION]\n",
    "                {question}\n",
    "                [TASK]\n",
    "                Integrate all informations from every summary, resolve contradictions, and provide a comprehensive answer.\n",
    "                Answer format: <answer>[Combined response using all relevant facts]</answer>\n",
    "                \"\"\"\n",
    "            )\n",
    "        elif task_type == TaskType.SUMMARIZATION:\n",
    "            return TaskConfig(\n",
    "                first_worker_instruction=\"\"\"\n",
    "                [CONTEXT]\n",
    "                {chunk_text}\n",
    "                [TASK]\n",
    "                Create a lengthy summary by incorporating key information from this context.\n",
    "                Focus on main ideas, findings, and conclusions. Maintain a coherent narrative.\n",
    "                \"\"\",\n",
    "                worker_instruction=\"\"\"\n",
    "                [PREVIOUS SUMMARY]\n",
    "                {previous_summary}\n",
    "                [CONTEXT]\n",
    "                {chunk_text}\n",
    "                [TASK]\n",
    "                Refine the existing summary by incorporating key information from this context.\n",
    "                Focus on main ideas, findings, and conclusions. Maintain a coherent narrative.\n",
    "                \"\"\",\n",
    "                manager_instruction=\"\"\"\n",
    "                [SUMMARY]\n",
    "                {last_summary}\n",
    "                [TASK]\n",
    "                Using all context available, resolve any contradictions and only provide a single lengthy summary using simple, everyday language.\n",
    "                Format: <summary>...</summary>\n",
    "                \"\"\",\n",
    "                multi_summary_instruction=\"\"\"\n",
    "                [SUMMARIES]\n",
    "                {last_summary}\n",
    "                [TASK]\n",
    "                Integrate all information from the summaries into a single coherent summary.\n",
    "                Resolve any contradictions and ensure the logical flow of ideas.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "class ProcessingMode(Enum):\n",
    "    LTR = \"left_to_right\"\n",
    "    RTL = \"right_to_left\"\n",
    "    RAND = \"random\"\n",
    "\n",
    "@dataclass\n",
    "class TextChunk:\n",
    "    text: str\n",
    "    chunk_id: str = \"-1\"\n",
    "    left_child: Optional['TextChunk'] = None\n",
    "    right_child: Optional['TextChunk'] = None\n",
    "    depth: int = 0\n",
    "    token_count: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ChainOfAgentsConfig:\n",
    "    worker_context_window: int = 16384\n",
    "    manager_context_window: int = 16384\n",
    "    max_tokens_per_chunk: int = 8192\n",
    "    processing_mode: ProcessingMode = ProcessingMode.LTR\n",
    "    task_type: TaskType = TaskType.QA\n",
    "    split_threshold: float = 1.1  # Threshold for splitting chunks based on priority score\n",
    "    sensitivity_curve: float = 0.3 \n",
    "    min_tokens_to_split: int = 512 \n",
    "\n",
    "class Model(LLM):\n",
    "    model: str = Field(\n",
    "        default=\"NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
    "        description=\"The name/path of the model to use\"\n",
    "    )\n",
    "    max_tokens_response: int = Field(\n",
    "        default=2048,\n",
    "        description=\"Maximum number of tokens in response\"\n",
    "    )\n",
    "    instruction_format: str = Field(\n",
    "        default=\"llama\",\n",
    "        description=\"Instruction format to use: 'mistral' or 'llama'\"\n",
    "    )\n",
    "    api_url: str = Field(\n",
    "        default=\"http://localhost:8000/v1/completions\",\n",
    "        description=\"URL of the TabbyAPI endpoint\"\n",
    "    )\n",
    "    api_key: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"API key for authentication with TabbyAPI\"\n",
    "    )\n",
    "    context_window: int = Field(\n",
    "        default=16384,\n",
    "        description=\"Context window for the worker/manager\"\n",
    "    )\n",
    "    _client: httpx.AsyncClient = PrivateAttr()\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._client = httpx.AsyncClient()\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model,\n",
    "            use_fast=True\n",
    "        )\n",
    "        \n",
    "    def format_prompt(self, instruction: str) -> str:\n",
    "        instruction = instruction.strip()\n",
    "        if self.instruction_format.lower() == \"llama\":\n",
    "            return (\n",
    "                f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"\n",
    "                f\"{instruction}\"\n",
    "                f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )\n",
    "        return f\"<s>[INST]{instruction}[/INST]\"\n",
    "\n",
    "\n",
    "    async def complete(self, prompt: str) -> str:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        \n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": self.max_tokens_response,\n",
    "            \"temperature\": 0.0,\n",
    "            \"temperature_last\": False,\n",
    "            \"dynamic_temperature\": False,\n",
    "            \"dynamic_temperature_low\": 0.1,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"min_p\": 0.0,\n",
    "            \"top_k\": 0,\n",
    "            \"typical_p\": 1.0,\n",
    "            \"tfs\": 1.0,\n",
    "            \"top_a\": 0.0,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"min_new_tokens\": 200,\n",
    "            \"no_repeat_ngram_size\": 0,\n",
    "            \"num_beams\": 1,\n",
    "            \"early_stopping\": False,\n",
    "            \"seed\": 0,\n",
    "            \"add_bos_token\": True,\n",
    "            \"truncation_length\": self.context_window,\n",
    "            \"ban_eos_token\": False,\n",
    "            \"skip_special_tokens\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"Making request to {self.api_url}\")\n",
    "        print(f\"Payload size: {len(str(payload))} characters\")\n",
    "    \n",
    "        try:\n",
    "            print(\"Initiating API request...\")\n",
    "            response = await self._client.post(\n",
    "                f\"{self.api_url}/v1/completions\",\n",
    "                json=payload,\n",
    "                headers=headers,\n",
    "                timeout=900.0\n",
    "            )\n",
    "            print(f\"Request completed with status code: {response.status_code}\")\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            print(\"Response received and parsed successfully\")\n",
    "            return result['choices'][0]['text'].strip()\n",
    "            \n",
    "        except httpx.ReadTimeout as e:\n",
    "            print(f\"Read timeout error occurred: {str(e)}\")\n",
    "            print(f\"The server took too long to send the response\")\n",
    "            raise Exception(f'API read timeout: {str(e)}')\n",
    "            \n",
    "        except httpx.ConnectTimeout as e:\n",
    "            print(f\"Connection timeout error occurred: {str(e)}\")\n",
    "            print(f\"Could not establish connection to the server\")\n",
    "            raise Exception(f'API connection timeout: {str(e)}')\n",
    "            \n",
    "        except httpx.RequestError as e:\n",
    "            print(f\"Request error: {str(e)}\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            print(f\"Request details: {e.request.url if e.request else 'No request info'}\")\n",
    "            raise Exception(f'API request error: {str(e)}')\n",
    "            \n",
    "        except httpx.HTTPStatusError as e:\n",
    "            print(f\"HTTP error {e.response.status_code}\")\n",
    "            print(f\"Response text: {e.response.text}\")\n",
    "            print(f\"Request URL: {e.request.url}\")\n",
    "            print(f\"Request headers: {e.request.headers}\")\n",
    "            raise Exception(f'API HTTP error: {str(e)}')\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in response: {str(e)}\")\n",
    "            print(f\"Full response: {result}\")\n",
    "            raise Exception(f'API response error: {str(e)}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            import traceback\n",
    "            print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "            raise Exception(f'API error: {str(e)}')\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"Close the HTTP client explicitly\"\"\"\n",
    "        await self._client.aclose()\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return self._tokenizer\n",
    "\n",
    "class WorkerAgent(Assistant):    \n",
    "    def __init__(self, llm: Model, chunk_id: str):  # chunk_id is now a string\n",
    "        super().__init__(\n",
    "            name=f\"worker_{chunk_id}\",\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "    async def process_chunk(\n",
    "        self,\n",
    "        chunk: TextChunk,\n",
    "        previous_summary: Optional[str], \n",
    "        query: Optional[str],\n",
    "        instruction: str\n",
    "    ) -> str:\n",
    "        formatted_instruction = instruction.format(\n",
    "            chunk_text=chunk.text,\n",
    "            previous_summary=previous_summary or \"No previous summary\",\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        prompt = self.llm.format_prompt(formatted_instruction)\n",
    "                    \n",
    "        print(f\"\\n=== Worker Agent {chunk.chunk_id} Processing ===\")\n",
    "        response = await self.llm.complete(prompt)\n",
    "        print(f\"Worker Response:\\n{response}\\n\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "class ManagerAgent(Assistant):    \n",
    "    def __init__(self, llm: Model):\n",
    "        super().__init__(\n",
    "            name=\"manager\",\n",
    "            llm=llm\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_answer_tags(response: str, task_type: TaskType) -> str:\n",
    "        \"\"\"Remove answer tags from response if they exist and if task type is QA.\"\"\"\n",
    "        if task_type == TaskType.QA:\n",
    "            # Try to find content within <answer> tags\n",
    "            answer_match = re.search(r'<answer>(.*?)</answer>', response, re.IGNORECASE | re.DOTALL)\n",
    "            if answer_match:\n",
    "                answer_content = answer_match.group(1).strip()\n",
    "                \n",
    "                # Find any text after the closing </answer> tag\n",
    "                after_tag_match = re.search(r'</answer>(.*)', response, re.IGNORECASE | re.DOTALL)\n",
    "                result = answer_content\n",
    "                if after_tag_match and after_tag_match.group(1).strip():\n",
    "                    # Combine the answer content with the text after the tag\n",
    "                    result = f\"{answer_content} {after_tag_match.group(1).strip()}\"\n",
    "                \n",
    "                # Remove any remaining <answer> or </answer> tags\n",
    "                result = re.sub(r'</?answer>', '', result, flags=re.IGNORECASE)\n",
    "                return result\n",
    "        elif task_type == TaskType.SUMMARIZATION:\n",
    "            # Try to find content within <summary> tags\n",
    "            summary_match = re.search(r'<summary>(.*?)</summary>', response, re.IGNORECASE | re.DOTALL)\n",
    "            if summary_match:\n",
    "                summary_content = summary_match.group(1).strip()\n",
    "                \n",
    "                # Find any text after the closing </summary> tag\n",
    "                after_tag_match = re.search(r'</summary>(.*)', response, re.IGNORECASE | re.DOTALL)\n",
    "                result = summary_content\n",
    "                if after_tag_match and after_tag_match.group(1).strip():\n",
    "                    # Combine the summary content with the text after the tag\n",
    "                    result = f\"{summary_content} {after_tag_match.group(1).strip()}\"\n",
    "                \n",
    "                # Remove any remaining <summary> or </summary> tags\n",
    "                result = re.sub(r'</?summary>', '', result, flags=re.IGNORECASE)\n",
    "                return result\n",
    "                    \n",
    "        # If no tags are found, remove any leftover tags and return\n",
    "        result = re.sub(r'</?answer>|</?summary>', '', response, flags=re.IGNORECASE)\n",
    "        return result.strip()\n",
    "    \n",
    "    async def generate_response(\n",
    "        self,\n",
    "        summary: str,\n",
    "        query: Optional[str],\n",
    "        instruction: str,\n",
    "        task_type: TaskType\n",
    "    ) -> str:\n",
    "        formatted_instruction = instruction.format(\n",
    "            last_summary=summary,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        prompt = self.llm.format_prompt(formatted_instruction)\n",
    "        \n",
    "        print(\"\\n=== Manager Agent Processing ===\")\n",
    "        response = await self.llm.complete(prompt)\n",
    "        return ManagerAgent.remove_answer_tags(response, task_type)\n",
    "\n",
    "class ChunkProcessor:\n",
    "    def __init__(self, llm: Model, config: ChainOfAgentsConfig):\n",
    "        self.llm = llm\n",
    "        self.config = config\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.mean_score: float = 0.0\n",
    "        self.std_score: float = 0.0\n",
    "\n",
    "    def calculate_entropy(self, text: str) -> float:\n",
    "        \"\"\"Calculate Shannon entropy for text diversity\"\"\"\n",
    "        words = text.split()\n",
    "        total_words = len(words)\n",
    "        if total_words == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        _, counts = np.unique(words, return_counts=True)\n",
    "        probabilities = counts / total_words\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "    def calculate_priority_score(self, chunk: TextChunk, query: str) -> float:\n",
    "        entropy = self.calculate_entropy(chunk.text)\n",
    "        if not query:\n",
    "            return entropy\n",
    "            \n",
    "        vectors = self.vectorizer.fit_transform([chunk.text, query])\n",
    "        similarity = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "        \n",
    "        return (0.7 * entropy * np.log1p(entropy)) + (0.3 * similarity ** 2)\n",
    "\n",
    "    def subsplit_chunk(self, chunk: TextChunk) -> TextChunk:\n",
    "        \"\"\"Split chunk into binary tree of sub-chunks\"\"\"\n",
    "        if not self._needs_split(chunk, None) or chunk.depth >= 3:  # MAX_DEPTH = 3\n",
    "            return chunk\n",
    "            \n",
    "        sentences = chunk.text.split('. ')\n",
    "        mid = len(sentences) // 2\n",
    "        \n",
    "        left_text = '. '.join(sentences[:mid]) + '.'\n",
    "        right_text = '. '.join(sentences[mid:]) + '.'\n",
    "        \n",
    "        left_chunk = TextChunk(\n",
    "            text=left_text,\n",
    "            chunk_id=f\"{chunk.chunk_id}.1\",\n",
    "            depth=chunk.depth + 1\n",
    "        )\n",
    "        right_chunk = TextChunk(\n",
    "            text=right_text,\n",
    "            chunk_id=f\"{chunk.chunk_id}.2\",\n",
    "            depth=chunk.depth + 1\n",
    "        )\n",
    "        \n",
    "        chunk.left_child = self.subsplit_chunk(left_chunk)\n",
    "        chunk.right_child = self.subsplit_chunk(right_chunk)\n",
    "        \n",
    "        return chunk\n",
    "\n",
    "    def get_ordered_chunks(self, root_chunk: TextChunk) -> List[TextChunk]:\n",
    "        \"\"\"Get chunks in processing order (deepest first, left to right)\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        def traverse(chunk: TextChunk):\n",
    "            if not chunk:\n",
    "                return\n",
    "                \n",
    "            if not chunk.left_child and not chunk.right_child:\n",
    "                chunks.append(chunk)\n",
    "                return\n",
    "                \n",
    "            if chunk.left_child:\n",
    "                traverse(chunk.left_child)\n",
    "            if chunk.right_child:\n",
    "                traverse(chunk.right_child)\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        traverse(root_chunk)\n",
    "        return chunks\n",
    "\n",
    "    def _needs_split(self, chunk: TextChunk, query: Optional[str], min_tokens: int = 512) -> bool:\n",
    "        approx_tokens = chunk.token_count\n",
    "        MIN_TOKENS = min_tokens\n",
    "        \n",
    "        if approx_tokens < MIN_TOKENS:\n",
    "            print(f\"Chunk {chunk.chunk_id} below minimum token threshold ({approx_tokens} < {MIN_TOKENS})\")\n",
    "            return False\n",
    "            \n",
    "        score = self.calculate_priority_score(chunk, query) if query else self.calculate_entropy(chunk.text)\n",
    "        \n",
    "        distribution_factor = 1 - np.exp(-self.config.sensitivity_curve * self.std_score)\n",
    "        dynamic_threshold = self.mean_score + (self.config.split_threshold * distribution_factor * self.std_score)\n",
    "        needs_split = score > dynamic_threshold\n",
    "    \n",
    "        print(f\"Chunk {chunk.chunk_id} evaluation:\")\n",
    "        print(f\"  Score: {score:.2f}\")\n",
    "        print(f\"  Threshold: {dynamic_threshold:.2f} (mean {self.mean_score:.2f} + {self.config.split_threshold}σ)\")\n",
    "        \n",
    "        return needs_split\n",
    "    \n",
    "    def _split_into_chunks(self, text: str) -> List[TextChunk]:\n",
    "        # Get actual token count using the tokenizer\n",
    "        tokens = self.llm.tokenizer.encode(text)\n",
    "        total_tokens = len(tokens)\n",
    "        del tokens  # Free memory\n",
    "    \n",
    "        print(f\"\\n=== Document Token Count ===\")\n",
    "        print(f\"The document contains {total_tokens} tokens\")\n",
    "        \n",
    "        if total_tokens <= self.config.max_tokens_per_chunk:\n",
    "            print(\"Document not split because total_tokens <= max_tokens_per_chunk\")\n",
    "            return [TextChunk(text=text, chunk_id=\"0\", token_count=total_tokens)]\n",
    "    \n",
    "        segments = text.split('\\n')\n",
    "        if len(segments) <= 1:\n",
    "            # Split by sentences using compiled regex\n",
    "            segments = SENTENCE_SPLIT_PATTERN.split(text)\n",
    "            print(f\"Re-split into {len(segments)} sentence segments\")\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for segment in segments:\n",
    "            # Use actual token count instead of approximation\n",
    "            segment_tokens = len(self.llm.tokenizer.encode(segment))\n",
    "            \n",
    "            if current_size + segment_tokens > self.config.max_tokens_per_chunk:\n",
    "                if current_chunk:\n",
    "                    chunk_text = '\\n'.join(current_chunk)\n",
    "                    # Calculate actual token count for the complete chunk\n",
    "                    chunk_token_count = len(self.llm.tokenizer.encode(chunk_text))\n",
    "                    chunks.append(TextChunk(\n",
    "                        text=chunk_text,\n",
    "                        chunk_id=str(chunk_id),\n",
    "                        token_count=chunk_token_count  # Set the token count here\n",
    "                    ))\n",
    "                    chunk_id += 1\n",
    "                    current_chunk = [segment]\n",
    "                    current_size = segment_tokens\n",
    "                else:\n",
    "                    # Calculate actual token count for the complete chunk\n",
    "                    chunk_token_count = len(self.llm.tokenizer.encode(segment))\n",
    "                    chunks.append(TextChunk(\n",
    "                        text=segment,\n",
    "                        chunk_id=str(chunk_id),\n",
    "                        token_count=chunk_token_count  # Set the token count here\n",
    "                    ))\n",
    "                    chunk_id += 1\n",
    "            else:\n",
    "                current_chunk.append(segment)\n",
    "                current_size += segment_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunk_text = '\\n'.join(current_chunk)\n",
    "            # Calculate actual token count for the final chunk\n",
    "            chunk_token_count = len(self.llm.tokenizer.encode(chunk_text))\n",
    "            chunks.append(TextChunk(\n",
    "                text=chunk_text,\n",
    "                chunk_id=str(chunk_id),\n",
    "                token_count=chunk_token_count  # Set the token count here\n",
    "            ))\n",
    "        \n",
    "        print(f\"Document split into {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "\n",
    "class ChainOfAgents:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Model,\n",
    "        chunks: List[TextChunk] = None,\n",
    "        config: ChainOfAgentsConfig = ChainOfAgentsConfig(),\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.config = config\n",
    "        self.chunks = chunks or []\n",
    "        self.is_first_chunk: bool = True\n",
    "        self.task_config = TaskFactory.create_task(config.task_type)\n",
    "        self.chunk_processor = ChunkProcessor(llm, config)\n",
    "\n",
    "    def _get_chunk_order(self, num_chunks: int) -> List[int]:\n",
    "        if self.config.processing_mode == ProcessingMode.LTR:\n",
    "            return list(range(num_chunks))\n",
    "        elif self.config.processing_mode == ProcessingMode.RTL:\n",
    "            return list(range(num_chunks - 1, -1, -1))\n",
    "        else:\n",
    "            return random.sample(range(num_chunks), num_chunks)\n",
    "\n",
    "    async def process(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "    ) -> str:\n",
    "        current_summary = None\n",
    "        self.is_first_chunk = True\n",
    "        \n",
    "        initial_scores = []\n",
    "        print(\"\\n=== Initial Chunk Scores ===\")\n",
    "        for chunk in self.chunks:\n",
    "            score = self.chunk_processor.calculate_priority_score(chunk, query) if query else self.chunk_processor.calculate_entropy(chunk.text)\n",
    "            initial_scores.append(score)\n",
    "            print(f\"Chunk {chunk.chunk_id}: {score:.2f}\")\n",
    "    \n",
    "        self.chunk_processor.mean_score = np.mean(initial_scores) if initial_scores else 0.0\n",
    "        self.chunk_processor.std_score = np.std(initial_scores) if initial_scores else 0.0\n",
    "        \n",
    "        q3 = np.percentile(initial_scores, 75)\n",
    "        iqr = q3 - np.percentile(initial_scores, 25)\n",
    "        self.chunk_processor.std_score = min(self.chunk_processor.std_score, iqr/1.35)\n",
    "        \n",
    "        print(f\"\\nDistribution stats:\")\n",
    "        print(f\"Mean: {self.chunk_processor.mean_score:.2f}\")\n",
    "        print(f\"Std: {self.chunk_processor.std_score:.2f}\")\n",
    "    \n",
    "        for chunk in self.chunks:\n",
    "            current_summary = await self._process_sub_chunk(chunk, current_summary, query)\n",
    "        \n",
    "        return current_summary\n",
    "\n",
    "    async def _process_sub_chunk(self, chunk: TextChunk, current_summary: str, query: Optional[str]) -> str:\n",
    "        MAX_DEPTH = 3\n",
    "        depth = len(chunk.chunk_id.split('.')) - 1\n",
    "        \n",
    "        # Get actual token count for the chunk\n",
    "        chunk_tokens = len(self.llm.tokenizer.encode(chunk.text))\n",
    "        chunk.token_count = chunk_tokens\n",
    "        \n",
    "        # Check splitting criteria\n",
    "        should_not_split = (\n",
    "            depth >= MAX_DEPTH or \n",
    "            chunk_tokens < self.config.min_tokens_to_split or\n",
    "            not self.chunk_processor._needs_split(chunk, query, self.config.min_tokens_to_split)\n",
    "        )\n",
    "        \n",
    "        if not should_not_split:\n",
    "            # Look ahead: Check if splitting would create valid child chunks\n",
    "            # Improved sentence splitting with better regex\n",
    "            sentences = SENTENCE_SPLIT_PATTERN.split(chunk.text)\n",
    "            \n",
    "            # Make sure we have enough sentences to make splitting worthwhile\n",
    "            if len(sentences) >= 4:\n",
    "                mid = len(sentences) // 2\n",
    "                left_text = ' '.join(sentences[:mid])\n",
    "                right_text = ' '.join(sentences[mid:])\n",
    "                \n",
    "                # Verify that both resulting chunks have reasonable sizes using actual token counts\n",
    "                left_tokens = len(self.llm.tokenizer.encode(left_text))\n",
    "                right_tokens = len(self.llm.tokenizer.encode(right_text))\n",
    "            \n",
    "                # Only proceed with splitting if both resulting chunks are large enough\n",
    "                if left_tokens >= self.config.min_tokens_to_split and right_tokens >= self.config.min_tokens_to_split:\n",
    "                    left_chunk = TextChunk(\n",
    "                        text=left_text,\n",
    "                        chunk_id=f\"{chunk.chunk_id}.1\",\n",
    "                        depth=depth + 1,\n",
    "                        token_count=left_tokens\n",
    "                    )\n",
    "                    right_chunk = TextChunk(\n",
    "                        text=right_text,\n",
    "                        chunk_id=f\"{chunk.chunk_id}.2\",\n",
    "                        depth=depth + 1,\n",
    "                        token_count=right_tokens\n",
    "                    )\n",
    "                    \n",
    "                    chunk.left_child = left_chunk\n",
    "                    chunk.right_child = right_chunk\n",
    "                    \n",
    "                    # Process left chunk first, then right chunk\n",
    "                    print(f\"  Decision: SPLIT\")\n",
    "                    summary = await self._process_sub_chunk(left_chunk, current_summary, query)\n",
    "                    final_summary = await self._process_sub_chunk(right_chunk, summary, query)\n",
    "                    \n",
    "                    return final_summary\n",
    "            \n",
    "            # If we get here, splitting would create invalid child chunks, so we don't split\n",
    "            should_not_split = True\n",
    "\n",
    "        # Process the chunk without splitting\n",
    "        if should_not_split:\n",
    "            print(f\"  Decision: NOT SPLIT\")\n",
    "            worker = WorkerAgent(self.llm, chunk.chunk_id)\n",
    "            instruction = self.task_config.first_worker_instruction if self.is_first_chunk else self.task_config.worker_instruction\n",
    "            self.is_first_chunk = False\n",
    "            return await worker.process_chunk(\n",
    "                chunk=chunk,\n",
    "                previous_summary=current_summary,\n",
    "                query=query,\n",
    "                instruction=instruction\n",
    "            )\n",
    "\n",
    "async def main(\n",
    "    worker_context_window: int = 16384,\n",
    "    manager_context_window: int = 16384,\n",
    "    max_tokens_per_chunk: int = 4096,\n",
    "    max_tokens_response: int = 1024,\n",
    "    instruction_format: str = \"llama\",\n",
    "    model: str = \"NousResearch/Meta-Llama-3.1-8B-Instruct\", # For the tokenizer\n",
    "    task_type: TaskType = TaskType.QA,\n",
    "    query: str = None,\n",
    "    min_tokens_to_split: int = 512\n",
    "):\n",
    "    llm = Model(\n",
    "        max_tokens_response=max_tokens_response,\n",
    "        context_window=worker_context_window,\n",
    "        instruction_format=instruction_format,\n",
    "        model=model,\n",
    "        api_url=\"https://15ef-34-105-107-13.ngrok-free.app\"\n",
    "    )\n",
    "    \n",
    "    modes = [\n",
    "        ProcessingMode.LTR,\n",
    "        #ProcessingMode.RTL\n",
    "        #*[ProcessingMode.RAND]*5\n",
    "    ]\n",
    "    \n",
    "    txt_url = \"https://openreview.net/pdf?id=LuCLf4BJsr\"\n",
    "    save_path = \"/kaggle/working/chain_of_agent.pdf\"\n",
    "\n",
    "    !wget -O {save_path} {txt_url}\n",
    "    \n",
    "    text = extract_text(save_path)\n",
    "    #query = \"According to the paper what's the best context size window for the agents?'\"\n",
    "    query = \"List all the datasets used in the paper.\"\n",
    "    \n",
    "    config = ChainOfAgentsConfig(\n",
    "        worker_context_window=worker_context_window,\n",
    "        manager_context_window=manager_context_window,\n",
    "        max_tokens_per_chunk=max_tokens_per_chunk,\n",
    "        task_type=task_type,\n",
    "        min_tokens_to_split=min_tokens_to_split\n",
    "    )\n",
    "    temp_processor = ChunkProcessor(llm, config)\n",
    "    initial_chunks = temp_processor._split_into_chunks(text)\n",
    "    \n",
    "    final_summaries = []\n",
    "    for mode in modes:\n",
    "        print(f\"\\n=== Processing with {mode.value} mode ===\")\n",
    "        config.processing_mode = mode\n",
    "        coa = ChainOfAgents(llm, initial_chunks, config)\n",
    "        chain_summary = await coa.process(query=query if task_type == TaskType.QA else None)\n",
    "        final_summaries.append(chain_summary)\n",
    "    \n",
    "    manager = ManagerAgent(llm)\n",
    "    if len(final_summaries) > 1:\n",
    "        combined_summaries = \"\\n\\n\".join(\n",
    "            f\"Summary n°{i+1}:\\n{summary}\"\n",
    "            for i, summary in enumerate(final_summaries))\n",
    "        final_response = await manager.generate_response(\n",
    "            summary=combined_summaries,\n",
    "            query=query if task_type == TaskType.QA else None,\n",
    "            instruction=coa.task_config.multi_summary_instruction,\n",
    "            task_type=task_type\n",
    "        )\n",
    "    else:\n",
    "        final_response = await manager.generate_response(\n",
    "            summary=final_summaries[0],\n",
    "            query=query if task_type == TaskType.QA else None,\n",
    "            instruction=coa.task_config.manager_instruction,\n",
    "            task_type=task_type\n",
    "        )\n",
    "    \n",
    "    print(\"\\n=== Final Manager Response ===\")\n",
    "    print(final_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    try:\n",
    "       asyncio.run(main())\n",
    "    finally:\n",
    "       gc.collect()\n",
    "       torch.cuda.empty_cache()\n",
    "       print(\"GPU memory has been cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

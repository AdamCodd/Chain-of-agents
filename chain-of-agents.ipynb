{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f83025",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-01T15:44:58.524673Z",
     "iopub.status.busy": "2025-03-01T15:44:58.524236Z",
     "iopub.status.idle": "2025-03-01T15:47:14.816080Z",
     "shell.execute_reply": "2025-03-01T15:47:14.814711Z"
    },
    "papermill": {
     "duration": 136.29939,
     "end_time": "2025-03-01T15:47:14.818200",
     "exception": false,
     "start_time": "2025-03-01T15:44:58.518810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyMuPDF\r\n",
      "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\r\n",
      "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m65.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: PyMuPDF\r\n",
      "Successfully installed PyMuPDF-1.25.3\r\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\r\n",
      "Collecting transformers\r\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting phidata\r\n",
      "  Downloading phidata-2.7.10-py3-none-any.whl.metadata (38 kB)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\r\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\r\n",
      "Requirement already satisfied: docstring-parser in /usr/local/lib/python3.10/dist-packages (from phidata) (0.16)\r\n",
      "Requirement already satisfied: gitpython in /usr/local/lib/python3.10/dist-packages (from phidata) (3.1.43)\r\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from phidata) (0.28.1)\r\n",
      "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from phidata) (2.11.0a2)\r\n",
      "Collecting pydantic-settings (from phidata)\r\n",
      "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\r\n",
      "Collecting python-dotenv (from phidata)\r\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\r\n",
      "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from phidata) (13.9.4)\r\n",
      "Requirement already satisfied: tomli in /usr/local/lib/python3.10/dist-packages (from phidata) (2.2.1)\r\n",
      "Requirement already satisfied: typer in /usr/local/lib/python3.10/dist-packages (from phidata) (0.15.1)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from phidata) (4.12.2)\r\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\r\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython->phidata) (4.0.11)\r\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->phidata) (3.7.1)\r\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->phidata) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->phidata) (1.0.7)\r\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->phidata) (3.10)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->phidata) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->phidata) (0.7.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->phidata) (2.29.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\r\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->phidata) (3.0.0)\r\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->phidata) (2.19.1)\r\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer->phidata) (8.1.7)\r\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer->phidata) (1.5.4)\r\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython->phidata) (5.0.1)\r\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->phidata) (0.1.2)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->phidata) (1.3.1)\r\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->phidata) (1.2.2)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\r\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading phidata-2.7.10-py3-none-any.whl (716 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m716.9/716.9 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\r\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\r\n",
      "Installing collected packages: python-dotenv, pydantic-settings, phidata, transformers\r\n",
      "  Attempting uninstall: transformers\r\n",
      "    Found existing installation: transformers 4.47.0\r\n",
      "    Uninstalling transformers-4.47.0:\r\n",
      "      Successfully uninstalled transformers-4.47.0\r\n",
      "Successfully installed phidata-2.7.10 pydantic-settings-2.8.1 python-dotenv-1.0.1 transformers-4.49.0\r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0be9da425cef4b87ac534e47a567d6f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/50.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2965cf30eec47d58bad0dc349930f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b45fc9e726354dd5997cfd4750d7d474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-01 15:45:55--  https://openreview.net/pdf?id=LuCLf4BJsr\r\n",
      "Resolving openreview.net (openreview.net)... 35.184.86.251\r\n",
      "Connecting to openreview.net (openreview.net)|35.184.86.251|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 2683436 (2.6M) [application/pdf]\r\n",
      "Saving to: ‘/kaggle/working/chain_of_agent.pdf’\r\n",
      "\r\n",
      "/kaggle/working/cha 100%[===================>]   2.56M  10.8MB/s    in 0.2s    \r\n",
      "\r\n",
      "2025-03-01 15:45:56 (10.8 MB/s) - ‘/kaggle/working/chain_of_agent.pdf’ saved [2683436/2683436]\r\n",
      "\r\n",
      "\n",
      "=== Document Token Count ===\n",
      "The document contains 17433 tokens\n",
      "Document split into 5 chunks\n",
      "\n",
      "=== Processing with left_to_right mode ===\n",
      "\n",
      "=== Initial Chunk Scores ===\n",
      "Chunk 0: 14.74\n",
      "Chunk 1: 14.28\n",
      "Chunk 2: 14.57\n",
      "Chunk 3: 13.85\n",
      "Chunk 4: 12.47\n",
      "\n",
      "Distribution stats:\n",
      "Mean: 13.98\n",
      "Std: 0.53\n",
      "  Decision: NOT SPLIT\n",
      "\n",
      "=== Worker Agent 0 Processing ===\n",
      "Making request to https://2506-34-83-136-23.ngrok-free.app\n",
      "Payload size: 18647 characters\n",
      "Initiating API request...\n",
      "Request completed with status code: 200\n",
      "Response received and parsed successfully\n",
      "Worker Response:\n",
      "Here are the answers:\n",
      "\n",
      "**Datasets**\n",
      "\n",
      "* HotpotQA\n",
      "* MuSiQue\n",
      "* NarrativeQA\n",
      "* Qasper\n",
      "* QuALITY\n",
      "* QMSum\n",
      "* GovReport\n",
      "* BookSum\n",
      "* RepoBench-P\n",
      "\n",
      "**Task Facts**\n",
      "\n",
      "* Fact 1: The paper proposes a new framework called Chain-of-Agents (CoA) for addressing long-context tasks.\n",
      "* Fact 2: CoA involves multiple worker agents communicating sequentially to handle different segments of the text.\n",
      "* Fact 3: The manager agent synthesizes the contributions from the worker agents to generate the final output.\n",
      "* Fact 4: CoA aims to overcome the limitations of traditional input reduction and window extension approaches.\n",
      "* Fact 5: The paper conducts extensive experiments on nine datasets, comparing CoA with two strong baseline approaches.\n",
      "\n",
      "**Summary**\n",
      "The paper introduces Chain-of-Agents (CoA), a novel framework designed to address long-context tasks. CoA employs multiple worker agents to segment and comprehend the input text, followed by sequential communication and integration of findings by the manager agent. The framework seeks to overcome the limitations of traditional input reduction and window extension approaches, offering improved performance on long-context tasks compared to strong baseline approaches.\n",
      "\n",
      "  Decision: NOT SPLIT\n",
      "\n",
      "=== Worker Agent 1 Processing ===\n",
      "Making request to https://2506-34-83-136-23.ngrok-free.app\n",
      "Payload size: 17131 characters\n",
      "Initiating API request...\n",
      "Request completed with status code: 200\n",
      "Response received and parsed successfully\n",
      "Worker Response:\n",
      "Here's my attempt to summarize the key points:\n",
      "\n",
      "**Datasets**: \n",
      "\n",
      "1. HotpotQA \n",
      "2. MuSiQue \n",
      "3. NarrativeQA \n",
      "4. Qasper \n",
      "5. QuALITY \n",
      "6. QMSum \n",
      "7. GovReport \n",
      "8. BookSum \n",
      "9. RepoBench-P\n",
      "\n",
      "**Facts**:\n",
      "\n",
      "1. The paper proposes a new framework called Chain-of-Agents (CoA) for addressing long-context tasks.\n",
      "2. CoA involves multiple worker agents communicating sequentially to handle different segments of the text.\n",
      "3. The manager agent synthesizes the contributions from the worker agents to generate the final output.\n",
      "4. CoA aims to overcome the limitations of traditional input reduction and window extension approaches.\n",
      "5. The paper conducts extensive experiments on nine datasets, comparing CoA with two strong baseline approaches.\n",
      "\n",
      "**Revised Summary**: Not answered since I'm supposed to provide a revised summary instead!\n",
      "\n",
      "  Decision: NOT SPLIT\n",
      "\n",
      "=== Worker Agent 2 Processing ===\n",
      "Making request to https://2506-34-83-136-23.ngrok-free.app\n",
      "Payload size: 18676 characters\n",
      "Initiating API request...\n",
      "Request completed with status code: 200\n",
      "Response received and parsed successfully\n",
      "Worker Response:\n",
      "Here are the facts extracted from the context:\n",
      "\n",
      "* Fact 1: The paper proposes a new framework called Chain-of-Agents (CoA) for addressing long-context tasks.\n",
      "* Fact 2: CoA involves multiple worker agents communicating sequentially to handle different segments of the text.\n",
      "* Fact 3: The manager agent synthesizes the contributions from the worker agents to generate the final output.\n",
      "* Fact 4: Nine datasets were used in the experiment, including HotpotQA, MuSiQue, NarrativeQA, Qasper, QuALITY, QMSum, GovReport, BookSum, and RepoBench-P.\n",
      "* Fact 5: The paper conducted an ablation study to demonstrate the importance of the manager agent.\n",
      "* Fact 6: Three alternative designs were explored, including bi-directional, self-consistent, and permutation-based approaches.\n",
      "* Fact 7: The paper analyzed the information loss during information propagation in CoUis.\n",
      "* Fact 8: The Chain-of-Agents framework was designed to mitigate lost-in-the-middle issues and perform better on longer samples.\n",
      "\n",
      "With these facts, here is the refined summary:\n",
      "\n",
      "The paper introduces Chain-of-Agents (CoA), a novel framework for tackling long-context tasks. CoA consists of multiple worker agents that communicate sequentially to process distinct parts of the text, followed by a manager agent that combines their outputs to generate the final answer. An ablation study reveals the significance of the manager agent, while alternative designs, such as bi-directional, self-consistent, and permutation-based approaches, were explored. The authors utilized nine datasets, including HotpotQA, MuSiQue, NarrativeQA, Qasper, QuALITY, QMSum, GovReport, BookSum, and RepoBench-P, to test the efficacy of CoA. Additionally, the paper discusses information loss during information propagation and how CoA addresses lost-in-the-middle issues to achieve better performance on longer samples.\n",
      "\n",
      "  Decision: NOT SPLIT\n",
      "\n",
      "=== Worker Agent 3 Processing ===\n",
      "Making request to https://2506-34-83-136-23.ngrok-free.app\n",
      "Payload size: 21791 characters\n",
      "Initiating API request...\n",
      "Request completed with status code: 200\n",
      "Response received and parsed successfully\n",
      "Worker Response:\n",
      "Here's your revised summary incorporating the new information:\n",
      "\n",
      "**Summary**\n",
      "\n",
      "The paper introduces Chain-of-Agents (CoA), a novel framework for tackling long-context tasks. CoA consists of multiple worker agents that communicate sequentially to process distinct parts of the text, followed by a manager agent that combines their outputs to generate the final answer. An ablation study reveals the significance of the manager agent, while alternative designs, such as bi-directional, self-consistent, and permutation-based approaches, were explored. The authors utilized nine datasets, including HotpotQA, MuSiQue, NarrativeQA, Qasper, QuALITY, QMSum, GovReport, BookSum, and RepoBench-P, to test the efficacy of CoA. Additionally, the paper discusses information loss during information propagation and how CoA addresses lost-in-the-middle issues to achieve better performance on longer samples.\n",
      "\n",
      "**Additional Information**\n",
      "\n",
      "Nine datasets were used in the experiment, including HotpotQA, MuSiQue, NarrativeQA, Qasper, QuALITY, QMSum, GovReport, BookSum, and RepoBench-P. The paper conducted an ablation study to demonstrate the importance of the manager agent. Alternative designs, such as bi-directional, self-consistent, and permutation-based approaches, were explored. The author's goal is to prevent catastrophic collapse, which occurs when certain irrelevant content is present in the communication unit, preventing the worker units from generating meaningful responses.\n",
      "\n",
      "  Decision: NOT SPLIT\n",
      "\n",
      "=== Worker Agent 4 Processing ===\n",
      "Making request to https://2506-34-83-136-23.ngrok-free.app\n",
      "Payload size: 11465 characters\n",
      "Initiating API request...\n",
      "Request completed with status code: 200\n",
      "Response received and parsed successfully\n",
      "Worker Response:\n",
      "Here are the facts that might help to answer the question:\n",
      "\n",
      "* Fact 1: Nine datasets were used in the experiment.\n",
      "* Fact 2: These datasets include HotpotQA, MuSiQue, NarrativeQA, Qasper, QuALITY, QMSum, GovReport, BookSum, and RepoBench-P.\n",
      "* Fact 3: The paper provides detailed information on the computer resources needed to reproduce the experiments.\n",
      "\n",
      "With these facts, I can now revise the summary to incorporate the new information:\n",
      "\n",
      "Revised Summary:\n",
      "\n",
      "The paper presents Chain-of-Agents (CoA), a novel framework for addressing long-context tasks. CoA comprises multiple worker agents communicating sequentially to process different parts of the text, followed by a manager agent combining their outputs to produce the final response. A thorough analysis demonstrates the crucial role played by the manager agent. To explore various design alternatives, the researchers examined bi-directional, self-consistent, and permutation-based approaches. In testing the effectiveness of CoA, the team employed nine diverse datasets, including HotpotQA, MuSiQue, NarrativeQA, Qasper, QuALITY, QMSum, GovReport, BookSum, and RepoBench-P. Furthermore, the paper highlights the issue of information loss during transmission and explains how CoA mitigates \"lost-in-the-middle\" problems to yield improved outcomes for longer input sequences.\n",
      "\n",
      "\n",
      "=== Manager Agent Processing ===\n",
      "Making request to https://2506-34-83-136-23.ngrok-free.app\n",
      "Payload size: 2263 characters\n",
      "Initiating API request...\n",
      "Request completed with status code: 200\n",
      "Response received and parsed successfully\n",
      "\n",
      "=== Final Manager Response ===\n",
      "The following datasets were used in the paper:\n",
      "HotpotQA\n",
      "MuSiQue\n",
      "NarrativeQA\n",
      "Qasper\n",
      "QuALITY\n",
      "QMSum\n",
      "GovReport\n",
      "BookSum\n",
      "RepoBench-P\n",
      "GPU memory has been cleared.\n"
     ]
    }
   ],
   "source": [
    "!pip install PyMuPDF\n",
    "!pip install -U transformers phidata\n",
    "\n",
    "from typing import List, Optional\n",
    "from dataclasses import dataclass\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from phi.assistant import Assistant\n",
    "from phi.llm.base import LLM\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import re\n",
    "import fitz\n",
    "import gc\n",
    "import random\n",
    "import httpx\n",
    "from pydantic import Field, PrivateAttr\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "SENTENCE_SPLIT_PATTERN = re.compile(r'(?<=[.!?])\\s+')\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "def extract_text_with_pymupdf(pdf_path):\n",
    "    references_pattern = re.compile(r'^references?$|^bibliography$')\n",
    "    citation_pattern = re.compile(r'^\\[\\d+\\]|^\\d+\\.|^[A-Z][a-z]+,\\s*[A-Z]\\.|^\\s*[A-Z][a-z]+\\s+[A-Z][a-z]+')\n",
    "    number_pattern = re.compile(r'^\\[\\d+\\]$|^\\d+\\.$')\n",
    "    year_pattern = re.compile(r'^.*?\\(\\d{4}\\)[.,].*?$')\n",
    "    whitespace_pattern = re.compile(r'\\s+')\n",
    "    \n",
    "    full_text = []\n",
    "    in_references = False\n",
    "    \n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                blocks = page.get_text(\"blocks\")\n",
    "                blocks.sort(key=lambda b: (b[1], b[0]))\n",
    "                \n",
    "                for block in blocks:\n",
    "                    text = block[4].strip() if block[4] else ''\n",
    "                    if not text:\n",
    "                        continue\n",
    "                        \n",
    "                    if references_pattern.match(text.lower()):\n",
    "                        in_references = True\n",
    "                        continue\n",
    "                        \n",
    "                    if in_references and citation_pattern.match(text):\n",
    "                        continue\n",
    "                    \n",
    "                    if number_pattern.match(text):\n",
    "                        continue\n",
    "                        \n",
    "                    if year_pattern.match(text) and len(text.split()) < 20:\n",
    "                        continue\n",
    "                    \n",
    "                    text = whitespace_pattern.sub(' ', text)\n",
    "                    if text.isupper() or len(text) <= 3:\n",
    "                        full_text.append(f\"\\n## {text}\\n\")\n",
    "                    else:\n",
    "                        full_text.append(text + \"\\n\")\n",
    "    except Exception as e:\n",
    "        raise ValueError(f\"Error processing PDF file: {str(e)}\")\n",
    "    \n",
    "    return \"\\n\".join(full_text)\n",
    "\n",
    "def extract_text(file_path):\n",
    "    if file_path.lower().endswith('.txt'):\n",
    "        return read_text_file(file_path)\n",
    "    elif file_path.lower().endswith('.pdf'):\n",
    "        return extract_text_with_pymupdf(file_path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format. File must be either .pdf or .txt\")\n",
    "\n",
    "@dataclass\n",
    "class TaskConfig:\n",
    "    first_worker_instruction: str\n",
    "    worker_instruction: str\n",
    "    manager_instruction: str\n",
    "    multi_summary_instruction: str\n",
    "\n",
    "class TaskType(Enum):\n",
    "    QA = \"qa\"\n",
    "    SUMMARIZATION = \"summarization\"\n",
    "\n",
    "class TaskFactory:\n",
    "    @staticmethod\n",
    "    def create_task(task_type: TaskType) -> TaskConfig:\n",
    "        if task_type == TaskType.QA:\n",
    "            return TaskConfig(\n",
    "                first_worker_instruction=\"\"\"\n",
    "                [CONTEXT]\n",
    "                {chunk_text}\n",
    "                [QUESTION]\n",
    "                {question}\n",
    "                [TASK]\n",
    "                List several facts from the provided context that might help to answer the question:\n",
    "                - [Fact 1]\n",
    "                - [Fact 2]\n",
    "                ...\n",
    "                Provide a [SUMMARY] by summarizing all the relevant information related to the question.\n",
    "                Keep the existing structure in focus. Do not answer the question.\n",
    "                \"\"\",\n",
    "                worker_instruction=\"\"\"\n",
    "                [SUMMARY]\n",
    "                {previous_summary}\n",
    "                [CONTEXT]\n",
    "                {chunk_text}\n",
    "                [QUESTION]\n",
    "                {question}\n",
    "                [TASK]\n",
    "                List several facts from the provided context that might help to answer the question:\n",
    "                - [Fact 1]\n",
    "                - [Fact 2]\n",
    "                ...\n",
    "                Prioritize all relevant information to the question and then refine the current summary by including the new additional information in [REVISED SUMMARY].\n",
    "                Do not answer the question.\n",
    "                \"\"\",\n",
    "                manager_instruction=\"\"\"\n",
    "                [SUMMARY]\n",
    "                {last_summary}\n",
    "                [QUESTION]\n",
    "                {question}\n",
    "                [TASK]\n",
    "                Using all context available, resolve any contradictions, and provide a comprehensive answer below. Avoid lengthy explanations.\n",
    "                Answer format: <answer>...</answer>\n",
    "                \"\"\",\n",
    "                multi_summary_instruction=\"\"\"\n",
    "                [SUMMARIES]\n",
    "                {last_summary}\n",
    "                [QUESTION]\n",
    "                {question}\n",
    "                [TASK]\n",
    "                Integrate all informations from every summary, resolve contradictions, and provide a comprehensive answer.\n",
    "                Answer format: <answer>[Combined response using all relevant facts]</answer>\n",
    "                \"\"\"\n",
    "            )\n",
    "        elif task_type == TaskType.SUMMARIZATION:\n",
    "            return TaskConfig(\n",
    "                first_worker_instruction=\"\"\"\n",
    "                [CONTEXT]\n",
    "                {chunk_text}\n",
    "                [TASK]\n",
    "                Create a lengthy summary by incorporating key information from this context.\n",
    "                Focus on main ideas, findings, and conclusions. Maintain a coherent narrative.\n",
    "                \"\"\",\n",
    "                worker_instruction=\"\"\"\n",
    "                [PREVIOUS SUMMARY]\n",
    "                {previous_summary}\n",
    "                [CONTEXT]\n",
    "                {chunk_text}\n",
    "                [TASK]\n",
    "                Refine the existing summary by incorporating key information from this context.\n",
    "                Focus on main ideas, findings, and conclusions. Maintain a coherent narrative.\n",
    "                \"\"\",\n",
    "                manager_instruction=\"\"\"\n",
    "                [SUMMARY]\n",
    "                {last_summary}\n",
    "                [TASK]\n",
    "                Using all context available, resolve any contradictions and only provide a single lengthy summary using simple, everyday language.\n",
    "                Format: <summary>...</summary>\n",
    "                \"\"\",\n",
    "                multi_summary_instruction=\"\"\"\n",
    "                [SUMMARIES]\n",
    "                {last_summary}\n",
    "                [TASK]\n",
    "                Integrate all information from the summaries into a single coherent summary.\n",
    "                Resolve any contradictions and ensure the logical flow of ideas.\n",
    "                \"\"\"\n",
    "            )\n",
    "\n",
    "class ProcessingMode(Enum):\n",
    "    LTR = \"left_to_right\"\n",
    "    RTL = \"right_to_left\"\n",
    "    RAND = \"random\"\n",
    "\n",
    "@dataclass\n",
    "class TextChunk:\n",
    "    text: str\n",
    "    chunk_id: str = \"-1\"\n",
    "    left_child: Optional['TextChunk'] = None\n",
    "    right_child: Optional['TextChunk'] = None\n",
    "    depth: int = 0\n",
    "    token_count: int = 0\n",
    "\n",
    "@dataclass\n",
    "class ChainOfAgentsConfig:\n",
    "    worker_context_window: int = 16384\n",
    "    manager_context_window: int = 16384\n",
    "    max_tokens_per_chunk: int = 8192\n",
    "    processing_mode: ProcessingMode = ProcessingMode.LTR\n",
    "    task_type: TaskType = TaskType.QA\n",
    "    split_threshold: float = 1.1  # Threshold for splitting chunks based on priority score\n",
    "    sensitivity_curve: float = 0.3 \n",
    "    min_tokens_to_split: int = 512 \n",
    "\n",
    "class Model(LLM):\n",
    "    model: str = Field(\n",
    "        default=\"NousResearch/Meta-Llama-3.1-8B-Instruct\",\n",
    "        description=\"The name/path of the model to use\"\n",
    "    )\n",
    "    max_tokens_response: int = Field(\n",
    "        default=2048,\n",
    "        description=\"Maximum number of tokens in response\"\n",
    "    )\n",
    "    instruction_format: str = Field(\n",
    "        default=\"llama\",\n",
    "        description=\"Instruction format to use: 'mistral' or 'llama'\"\n",
    "    )\n",
    "    api_url: str = Field(\n",
    "        default=\"http://localhost:8000/v1/completions\",\n",
    "        description=\"URL of the TabbyAPI endpoint\"\n",
    "    )\n",
    "    api_key: Optional[str] = Field(\n",
    "        default=None,\n",
    "        description=\"API key for authentication with TabbyAPI\"\n",
    "    )\n",
    "    context_window: int = Field(\n",
    "        default=16384,\n",
    "        description=\"Context window for the worker/manager\"\n",
    "    )\n",
    "    _client: httpx.AsyncClient = PrivateAttr()\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self._client = httpx.AsyncClient()\n",
    "        self._tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model,\n",
    "            use_fast=True\n",
    "        )\n",
    "        \n",
    "    def format_prompt(self, instruction: str) -> str:\n",
    "        instruction = instruction.strip()\n",
    "        if self.instruction_format.lower() == \"llama\":\n",
    "            return (\n",
    "                f\"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\"\n",
    "                f\"{instruction}\"\n",
    "                f\"<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\n",
    "            )\n",
    "        return f\"<s>[INST]{instruction}[/INST]\"\n",
    "\n",
    "\n",
    "    async def complete(self, prompt: str) -> str:\n",
    "        headers = {\"Content-Type\": \"application/json\"}\n",
    "        \n",
    "        payload = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_tokens\": self.max_tokens_response,\n",
    "            \"temperature\": 0.0,\n",
    "            \"temperature_last\": False,\n",
    "            \"dynamic_temperature\": False,\n",
    "            \"dynamic_temperature_low\": 0.1,\n",
    "            \"do_sample\": True,\n",
    "            \"top_p\": 0.9,\n",
    "            \"min_p\": 0.0,\n",
    "            \"top_k\": 0,\n",
    "            \"typical_p\": 1.0,\n",
    "            \"tfs\": 1.0,\n",
    "            \"top_a\": 0.0,\n",
    "            \"repetition_penalty\": 1.2,\n",
    "            \"min_new_tokens\": 200,\n",
    "            \"no_repeat_ngram_size\": 0,\n",
    "            \"num_beams\": 1,\n",
    "            \"early_stopping\": False,\n",
    "            \"seed\": 0,\n",
    "            \"add_bos_token\": True,\n",
    "            \"truncation_length\": self.context_window,\n",
    "            \"ban_eos_token\": False,\n",
    "            \"skip_special_tokens\": True\n",
    "        }\n",
    "        \n",
    "        print(f\"Making request to {self.api_url}\")\n",
    "        print(f\"Payload size: {len(str(payload))} characters\")\n",
    "    \n",
    "        try:\n",
    "            print(\"Initiating API request...\")\n",
    "            response = await self._client.post(\n",
    "                f\"{self.api_url}/v1/completions\",\n",
    "                json=payload,\n",
    "                headers=headers,\n",
    "                timeout=900.0\n",
    "            )\n",
    "            print(f\"Request completed with status code: {response.status_code}\")\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            result = response.json()\n",
    "            \n",
    "            print(\"Response received and parsed successfully\")\n",
    "            return result['choices'][0]['text'].strip()\n",
    "            \n",
    "        except httpx.ReadTimeout as e:\n",
    "            print(f\"Read timeout error occurred: {str(e)}\")\n",
    "            print(f\"The server took too long to send the response\")\n",
    "            raise Exception(f'API read timeout: {str(e)}')\n",
    "            \n",
    "        except httpx.ConnectTimeout as e:\n",
    "            print(f\"Connection timeout error occurred: {str(e)}\")\n",
    "            print(f\"Could not establish connection to the server\")\n",
    "            raise Exception(f'API connection timeout: {str(e)}')\n",
    "            \n",
    "        except httpx.RequestError as e:\n",
    "            print(f\"Request error: {str(e)}\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            print(f\"Request details: {e.request.url if e.request else 'No request info'}\")\n",
    "            raise Exception(f'API request error: {str(e)}')\n",
    "            \n",
    "        except httpx.HTTPStatusError as e:\n",
    "            print(f\"HTTP error {e.response.status_code}\")\n",
    "            print(f\"Response text: {e.response.text}\")\n",
    "            print(f\"Request URL: {e.request.url}\")\n",
    "            print(f\"Request headers: {e.request.headers}\")\n",
    "            raise Exception(f'API HTTP error: {str(e)}')\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Missing key in response: {str(e)}\")\n",
    "            print(f\"Full response: {result}\")\n",
    "            raise Exception(f'API response error: {str(e)}')\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {str(e)}\")\n",
    "            print(f\"Error type: {type(e)}\")\n",
    "            import traceback\n",
    "            print(f\"Full traceback: {traceback.format_exc()}\")\n",
    "            raise Exception(f'API error: {str(e)}')\n",
    "\n",
    "    async def close(self):\n",
    "        \"\"\"Close the HTTP client explicitly\"\"\"\n",
    "        await self._client.aclose()\n",
    "    \n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return self._tokenizer\n",
    "\n",
    "class WorkerAgent(Assistant):    \n",
    "    def __init__(self, llm: Model, chunk_id: str):  # chunk_id is now a string\n",
    "        super().__init__(\n",
    "            name=f\"worker_{chunk_id}\",\n",
    "            llm=llm\n",
    "        )\n",
    "\n",
    "    async def process_chunk(\n",
    "        self,\n",
    "        chunk: TextChunk,\n",
    "        previous_summary: Optional[str], \n",
    "        query: Optional[str],\n",
    "        instruction: str\n",
    "    ) -> str:\n",
    "        formatted_instruction = instruction.format(\n",
    "            chunk_text=chunk.text,\n",
    "            previous_summary=previous_summary or \"No previous summary\",\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        prompt = self.llm.format_prompt(formatted_instruction)\n",
    "                    \n",
    "        print(f\"\\n=== Worker Agent {chunk.chunk_id} Processing ===\")\n",
    "        response = await self.llm.complete(prompt)\n",
    "        print(f\"Worker Response:\\n{response}\\n\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "class ManagerAgent(Assistant):    \n",
    "    def __init__(self, llm: Model):\n",
    "        super().__init__(\n",
    "            name=\"manager\",\n",
    "            llm=llm\n",
    "        )\n",
    "    \n",
    "    @staticmethod\n",
    "    def remove_answer_tags(response: str, task_type: TaskType) -> str:\n",
    "        \"\"\"Remove answer tags from response if they exist and if task type is QA.\"\"\"\n",
    "        if task_type == TaskType.QA:\n",
    "            matches = re.findall(r'<answer>(.*?)</answer>', response, re.IGNORECASE | re.DOTALL)\n",
    "            if matches:\n",
    "                return matches[0].strip()\n",
    "        elif task_type == TaskType.SUMMARIZATION:\n",
    "            matches = re.findall(r'<summary>(.*?)</summary>', response, re.IGNORECASE | re.DOTALL)\n",
    "            if matches:\n",
    "                return matches[0].strip()\n",
    "        return response.strip()\n",
    "    \n",
    "    async def generate_response(\n",
    "        self,\n",
    "        summary: str,\n",
    "        query: Optional[str],\n",
    "        instruction: str,\n",
    "        task_type: TaskType\n",
    "    ) -> str:\n",
    "        formatted_instruction = instruction.format(\n",
    "            last_summary=summary,\n",
    "            question=query\n",
    "        )\n",
    "        \n",
    "        prompt = self.llm.format_prompt(formatted_instruction)\n",
    "        \n",
    "        print(\"\\n=== Manager Agent Processing ===\")\n",
    "        response = await self.llm.complete(prompt)\n",
    "        return ManagerAgent.remove_answer_tags(response, task_type)\n",
    "\n",
    "class ChunkProcessor:\n",
    "    def __init__(self, llm: Model, config: ChainOfAgentsConfig):\n",
    "        self.llm = llm\n",
    "        self.config = config\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.mean_score: float = 0.0\n",
    "        self.std_score: float = 0.0\n",
    "\n",
    "    def calculate_entropy(self, text: str) -> float:\n",
    "        \"\"\"Calculate Shannon entropy for text diversity\"\"\"\n",
    "        words = text.split()\n",
    "        total_words = len(words)\n",
    "        if total_words == 0:\n",
    "            return 0.0\n",
    "            \n",
    "        _, counts = np.unique(words, return_counts=True)\n",
    "        probabilities = counts / total_words\n",
    "        return -np.sum(probabilities * np.log2(probabilities))\n",
    "\n",
    "    def calculate_priority_score(self, chunk: TextChunk, query: str) -> float:\n",
    "        entropy = self.calculate_entropy(chunk.text)\n",
    "        if not query:\n",
    "            return entropy\n",
    "            \n",
    "        vectors = self.vectorizer.fit_transform([chunk.text, query])\n",
    "        similarity = cosine_similarity(vectors[0], vectors[1])[0][0]\n",
    "        \n",
    "        return (0.7 * entropy * np.log1p(entropy)) + (0.3 * similarity ** 2)\n",
    "\n",
    "    def subsplit_chunk(self, chunk: TextChunk) -> TextChunk:\n",
    "        \"\"\"Split chunk into binary tree of sub-chunks\"\"\"\n",
    "        if not self._needs_split(chunk, None) or chunk.depth >= 3:  # MAX_DEPTH = 3\n",
    "            return chunk\n",
    "            \n",
    "        sentences = chunk.text.split('. ')\n",
    "        mid = len(sentences) // 2\n",
    "        \n",
    "        left_text = '. '.join(sentences[:mid]) + '.'\n",
    "        right_text = '. '.join(sentences[mid:]) + '.'\n",
    "        \n",
    "        left_chunk = TextChunk(\n",
    "            text=left_text,\n",
    "            chunk_id=f\"{chunk.chunk_id}.1\",\n",
    "            depth=chunk.depth + 1\n",
    "        )\n",
    "        right_chunk = TextChunk(\n",
    "            text=right_text,\n",
    "            chunk_id=f\"{chunk.chunk_id}.2\",\n",
    "            depth=chunk.depth + 1\n",
    "        )\n",
    "        \n",
    "        chunk.left_child = self.subsplit_chunk(left_chunk)\n",
    "        chunk.right_child = self.subsplit_chunk(right_chunk)\n",
    "        \n",
    "        return chunk\n",
    "\n",
    "    def get_ordered_chunks(self, root_chunk: TextChunk) -> List[TextChunk]:\n",
    "        \"\"\"Get chunks in processing order (deepest first, left to right)\"\"\"\n",
    "        chunks = []\n",
    "        \n",
    "        def traverse(chunk: TextChunk):\n",
    "            if not chunk:\n",
    "                return\n",
    "                \n",
    "            if not chunk.left_child and not chunk.right_child:\n",
    "                chunks.append(chunk)\n",
    "                return\n",
    "                \n",
    "            if chunk.left_child:\n",
    "                traverse(chunk.left_child)\n",
    "            if chunk.right_child:\n",
    "                traverse(chunk.right_child)\n",
    "            chunks.append(chunk)\n",
    "        \n",
    "        traverse(root_chunk)\n",
    "        return chunks\n",
    "\n",
    "    def _needs_split(self, chunk: TextChunk, query: Optional[str], min_tokens: int = 512) -> bool:\n",
    "        approx_tokens = chunk.token_count\n",
    "        MIN_TOKENS = min_tokens\n",
    "        \n",
    "        if approx_tokens < MIN_TOKENS:\n",
    "            print(f\"Chunk {chunk.chunk_id} below minimum token threshold ({approx_tokens} < {MIN_TOKENS})\")\n",
    "            return False\n",
    "            \n",
    "        score = self.calculate_priority_score(chunk, query) if query else self.calculate_entropy(chunk.text)\n",
    "        \n",
    "        distribution_factor = 1 - np.exp(-self.config.sensitivity_curve * self.std_score)\n",
    "        dynamic_threshold = self.mean_score + (self.config.split_threshold * distribution_factor * self.std_score)\n",
    "        needs_split = score > dynamic_threshold\n",
    "    \n",
    "        print(f\"Chunk {chunk.chunk_id} evaluation:\")\n",
    "        print(f\"  Score: {score:.2f}\")\n",
    "        print(f\"  Threshold: {dynamic_threshold:.2f} (mean {self.mean_score:.2f} + {self.config.split_threshold}σ)\")\n",
    "        \n",
    "        return needs_split\n",
    "    \n",
    "    def _split_into_chunks(self, text: str) -> List[TextChunk]:\n",
    "        # Get actual token count using the tokenizer\n",
    "        tokens = self.llm.tokenizer.encode(text)\n",
    "        total_tokens = len(tokens)\n",
    "        del tokens  # Free memory\n",
    "    \n",
    "        print(f\"\\n=== Document Token Count ===\")\n",
    "        print(f\"The document contains {total_tokens} tokens\")\n",
    "        \n",
    "        if total_tokens <= self.config.max_tokens_per_chunk:\n",
    "            print(\"Document not split because total_tokens <= max_tokens_per_chunk\")\n",
    "            return [TextChunk(text=text, chunk_id=\"0\", token_count=total_tokens)]\n",
    "    \n",
    "        segments = text.split('\\n')\n",
    "        if len(segments) <= 1:\n",
    "            # Split by sentences using compiled regex\n",
    "            segments = SENTENCE_SPLIT_PATTERN.split(text)\n",
    "            print(f\"Re-split into {len(segments)} sentence segments\")\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_size = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        for segment in segments:\n",
    "            segment_tokens = len(self.llm.tokenizer.encode(segment))\n",
    "            \n",
    "            if current_size + segment_tokens > self.config.max_tokens_per_chunk:\n",
    "                if current_chunk:\n",
    "                    chunk_text = '\\n'.join(current_chunk)\n",
    "                    # Calculate actual token count for the complete chunk\n",
    "                    chunk_token_count = len(self.llm.tokenizer.encode(chunk_text))\n",
    "                    chunks.append(TextChunk(\n",
    "                        text=chunk_text,\n",
    "                        chunk_id=str(chunk_id),\n",
    "                        token_count=chunk_token_count\n",
    "                    ))\n",
    "                    chunk_id += 1\n",
    "                    current_chunk = [segment]\n",
    "                    current_size = segment_tokens\n",
    "                else:\n",
    "                    # Calculate actual token count for the complete chunk\n",
    "                    chunk_token_count = len(self.llm.tokenizer.encode(segment))\n",
    "                    chunks.append(TextChunk(\n",
    "                        text=segment,\n",
    "                        chunk_id=str(chunk_id),\n",
    "                        token_count=chunk_token_count\n",
    "                    ))\n",
    "                    chunk_id += 1\n",
    "            else:\n",
    "                current_chunk.append(segment)\n",
    "                current_size += segment_tokens\n",
    "        \n",
    "        if current_chunk:\n",
    "            chunk_text = '\\n'.join(current_chunk)\n",
    "            # Calculate actual token count for the final chunk\n",
    "            chunk_token_count = len(self.llm.tokenizer.encode(chunk_text))\n",
    "            chunks.append(TextChunk(\n",
    "                text=chunk_text,\n",
    "                chunk_id=str(chunk_id),\n",
    "                token_count=chunk_token_count\n",
    "            ))\n",
    "        \n",
    "        print(f\"Document split into {len(chunks)} chunks\")\n",
    "        return chunks\n",
    "\n",
    "class ChainOfAgents:\n",
    "    def __init__(\n",
    "        self,\n",
    "        llm: Model,\n",
    "        chunks: List[TextChunk] = None,\n",
    "        config: ChainOfAgentsConfig = ChainOfAgentsConfig(),\n",
    "    ):\n",
    "        self.llm = llm\n",
    "        self.config = config\n",
    "        self.chunks = chunks or []\n",
    "        self.is_first_chunk: bool = True\n",
    "        self.task_config = TaskFactory.create_task(config.task_type)\n",
    "        self.chunk_processor = ChunkProcessor(llm, config)\n",
    "\n",
    "    def _get_chunk_order(self, num_chunks: int) -> List[int]:\n",
    "        if self.config.processing_mode == ProcessingMode.LTR:\n",
    "            return list(range(num_chunks))\n",
    "        elif self.config.processing_mode == ProcessingMode.RTL:\n",
    "            return list(range(num_chunks - 1, -1, -1))\n",
    "        else:\n",
    "            return random.sample(range(num_chunks), num_chunks)\n",
    "\n",
    "    async def process(\n",
    "        self,\n",
    "        query: Optional[str] = None,\n",
    "    ) -> str:\n",
    "        current_summary = None\n",
    "        self.is_first_chunk = True\n",
    "        \n",
    "        initial_scores = []\n",
    "        print(\"\\n=== Initial Chunk Scores ===\")\n",
    "        for chunk in self.chunks:\n",
    "            score = self.chunk_processor.calculate_priority_score(chunk, query) if query else self.chunk_processor.calculate_entropy(chunk.text)\n",
    "            initial_scores.append(score)\n",
    "            print(f\"Chunk {chunk.chunk_id}: {score:.2f}\")\n",
    "    \n",
    "        self.chunk_processor.mean_score = np.mean(initial_scores) if initial_scores else 0.0\n",
    "        self.chunk_processor.std_score = np.std(initial_scores) if initial_scores else 0.0\n",
    "        \n",
    "        q3 = np.percentile(initial_scores, 75)\n",
    "        iqr = q3 - np.percentile(initial_scores, 25)\n",
    "        self.chunk_processor.std_score = min(self.chunk_processor.std_score, iqr/1.35)\n",
    "        \n",
    "        print(f\"\\nDistribution stats:\")\n",
    "        print(f\"Mean: {self.chunk_processor.mean_score:.2f}\")\n",
    "        print(f\"Std: {self.chunk_processor.std_score:.2f}\")\n",
    "    \n",
    "        for chunk in self.chunks:\n",
    "            current_summary = await self._process_sub_chunk(chunk, current_summary, query)\n",
    "        \n",
    "        return current_summary\n",
    "\n",
    "    async def _process_sub_chunk(self, chunk: TextChunk, current_summary: str, query: Optional[str]) -> str:\n",
    "        MAX_DEPTH = 3\n",
    "        depth = len(chunk.chunk_id.split('.')) - 1\n",
    "        \n",
    "        # Get actual token count for the chunk\n",
    "        chunk_tokens = len(self.llm.tokenizer.encode(chunk.text))\n",
    "        chunk.token_count = chunk_tokens\n",
    "        \n",
    "        # Check splitting criteria\n",
    "        should_not_split = (\n",
    "            depth >= MAX_DEPTH or \n",
    "            chunk_tokens < self.config.min_tokens_to_split or\n",
    "            not self.chunk_processor._needs_split(chunk, query, self.config.min_tokens_to_split)\n",
    "        )\n",
    "        \n",
    "        if not should_not_split:\n",
    "            sentences = SENTENCE_SPLIT_PATTERN.split(chunk.text)\n",
    "            \n",
    "            # Make sure we have enough sentences to make splitting worthwhile\n",
    "            if len(sentences) >= 4:\n",
    "                mid = len(sentences) // 2\n",
    "                left_text = ' '.join(sentences[:mid])\n",
    "                right_text = ' '.join(sentences[mid:])\n",
    "                \n",
    "                # Verify that both resulting chunks have reasonable sizes using actual token counts\n",
    "                left_tokens = len(self.llm.tokenizer.encode(left_text))\n",
    "                right_tokens = len(self.llm.tokenizer.encode(right_text))\n",
    "            \n",
    "                # Only proceed with splitting if both resulting chunks are large enough\n",
    "                if left_tokens >= self.config.min_tokens_to_split and right_tokens >= self.config.min_tokens_to_split:\n",
    "                    left_chunk = TextChunk(\n",
    "                        text=left_text,\n",
    "                        chunk_id=f\"{chunk.chunk_id}.1\",\n",
    "                        depth=depth + 1,\n",
    "                        token_count=left_tokens\n",
    "                    )\n",
    "                    right_chunk = TextChunk(\n",
    "                        text=right_text,\n",
    "                        chunk_id=f\"{chunk.chunk_id}.2\",\n",
    "                        depth=depth + 1,\n",
    "                        token_count=right_tokens\n",
    "                    )\n",
    "                    \n",
    "                    chunk.left_child = left_chunk\n",
    "                    chunk.right_child = right_chunk\n",
    "                    \n",
    "                    # Process left chunk first, then right chunk\n",
    "                    print(f\"  Decision: SPLIT\")\n",
    "                    summary = await self._process_sub_chunk(left_chunk, current_summary, query)\n",
    "                    final_summary = await self._process_sub_chunk(right_chunk, summary, query)\n",
    "                    \n",
    "                    return final_summary\n",
    "            \n",
    "            # If we get here, splitting would create invalid child chunks, so we don't split\n",
    "            should_not_split = True\n",
    "\n",
    "        # Process the chunk without splitting\n",
    "        if should_not_split:\n",
    "            print(f\"  Decision: NOT SPLIT\")\n",
    "            worker = WorkerAgent(self.llm, chunk.chunk_id)\n",
    "            instruction = self.task_config.first_worker_instruction if self.is_first_chunk else self.task_config.worker_instruction\n",
    "            self.is_first_chunk = False\n",
    "            return await worker.process_chunk(\n",
    "                chunk=chunk,\n",
    "                previous_summary=current_summary,\n",
    "                query=query,\n",
    "                instruction=instruction\n",
    "            )\n",
    "\n",
    "async def main(\n",
    "    worker_context_window: int = 16384,\n",
    "    manager_context_window: int = 16384,\n",
    "    max_tokens_per_chunk: int = 4096,\n",
    "    max_tokens_response: int = 1024,\n",
    "    instruction_format: str = \"llama\",\n",
    "    model: str = \"NousResearch/Meta-Llama-3.1-8B-Instruct\", # For the tokenizer\n",
    "    task_type: TaskType = TaskType.QA,\n",
    "    query: str = None,\n",
    "    min_tokens_to_split: int = 512\n",
    "):\n",
    "    llm = Model(\n",
    "        max_tokens_response=max_tokens_response,\n",
    "        context_window=worker_context_window,\n",
    "        instruction_format=instruction_format,\n",
    "        model=model,\n",
    "        api_url=\"https://15ef-34-105-107-13.ngrok-free.app\"\n",
    "    )\n",
    "    \n",
    "    modes = [\n",
    "        ProcessingMode.LTR,\n",
    "        #ProcessingMode.RTL\n",
    "        #*[ProcessingMode.RAND]*5\n",
    "    ]\n",
    "    \n",
    "    txt_url = \"https://openreview.net/pdf?id=LuCLf4BJsr\"\n",
    "    save_path = \"/kaggle/working/chain_of_agent.pdf\"\n",
    "\n",
    "    !wget -O {save_path} {txt_url}\n",
    "    \n",
    "    text = extract_text(save_path)\n",
    "    #query = \"According to the paper what's the best context size window for the agents?'\"\n",
    "    query = \"List all the datasets used in the paper.\"\n",
    "    \n",
    "    config = ChainOfAgentsConfig(\n",
    "        worker_context_window=worker_context_window,\n",
    "        manager_context_window=manager_context_window,\n",
    "        max_tokens_per_chunk=max_tokens_per_chunk,\n",
    "        task_type=task_type,\n",
    "        min_tokens_to_split=min_tokens_to_split\n",
    "    )\n",
    "    temp_processor = ChunkProcessor(llm, config)\n",
    "    initial_chunks = temp_processor._split_into_chunks(text)\n",
    "    \n",
    "    final_summaries = []\n",
    "    for mode in modes:\n",
    "        print(f\"\\n=== Processing with {mode.value} mode ===\")\n",
    "        config.processing_mode = mode\n",
    "        coa = ChainOfAgents(llm, initial_chunks, config)\n",
    "        chain_summary = await coa.process(query=query if task_type == TaskType.QA else None)\n",
    "        final_summaries.append(chain_summary)\n",
    "    \n",
    "    manager = ManagerAgent(llm)\n",
    "    if len(final_summaries) > 1:\n",
    "        combined_summaries = \"\\n\\n\".join(\n",
    "            f\"Summary n°{i+1}:\\n{summary}\"\n",
    "            for i, summary in enumerate(final_summaries))\n",
    "        final_response = await manager.generate_response(\n",
    "            summary=combined_summaries,\n",
    "            query=query if task_type == TaskType.QA else None,\n",
    "            instruction=coa.task_config.multi_summary_instruction,\n",
    "            task_type=task_type\n",
    "        )\n",
    "    else:\n",
    "        final_response = await manager.generate_response(\n",
    "            summary=final_summaries[0],\n",
    "            query=query if task_type == TaskType.QA else None,\n",
    "            instruction=coa.task_config.manager_instruction,\n",
    "            task_type=task_type\n",
    "        )\n",
    "    \n",
    "    print(\"\\n=== Final Manager Response ===\")\n",
    "    print(final_response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    nest_asyncio.apply()\n",
    "    try:\n",
    "       asyncio.run(main())\n",
    "    finally:\n",
    "       gc.collect()\n",
    "       torch.cuda.empty_cache()\n",
    "       print(\"GPU memory has been cleared.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 142.670312,
   "end_time": "2025-03-01T15:47:18.336400",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-01T15:44:55.666088",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0be9da425cef4b87ac534e47a567d6f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_128343f799c7449dabb1ebfbf9322520",
        "IPY_MODEL_baa0f2a6b0704f76bbe94b358e099cb2",
        "IPY_MODEL_b1a89fb65e854b7b921fc52320fe951f"
       ],
       "layout": "IPY_MODEL_d63fc26476d5429e826b288764662dd2",
       "tabbable": null,
       "tooltip": null
      }
     },
     "0f46ec9d328742f6ae76dedf85c852f9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "10407867550e4c53a6e7a7e1feef1caf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "128343f799c7449dabb1ebfbf9322520": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_37568c46e6904544ad46eeedd31af20f",
       "placeholder": "​",
       "style": "IPY_MODEL_b8baa62dd5b74ea49a3a2ff365341e6e",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer_config.json: 100%"
      }
     },
     "16bb808a8a6c4bed8ccc0b111d2d602a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b461f09b4dda4625a30f52ed8ab765ef",
       "max": 9085657,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ce860d337f494e6f85c9de10617de011",
       "tabbable": null,
       "tooltip": null,
       "value": 9085657
      }
     },
     "201a3ed0208742648a1d406de51bcb1b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_cac9fd0263b1462381b99a50c7ade864",
       "placeholder": "​",
       "style": "IPY_MODEL_2b0621c5baf944a38b6434695d5fd3f5",
       "tabbable": null,
       "tooltip": null,
       "value": "tokenizer.json: 100%"
      }
     },
     "2b0621c5baf944a38b6434695d5fd3f5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "37568c46e6904544ad46eeedd31af20f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3cbcbfc46e2340f6bcde6b39f9c9f71c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_570d0898145046399920eaf09717204d",
       "placeholder": "​",
       "style": "IPY_MODEL_a2d71dd9596d46fa9074481ccb2067bd",
       "tabbable": null,
       "tooltip": null,
       "value": " 9.09M/9.09M [00:00&lt;00:00, 22.4MB/s]"
      }
     },
     "3da5fcaeb2ee4a5ea83657501f8bfe6a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "44a7c810214740ea9e66b5a55dea2ea4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "4d19635a3acd46e8914ce45db548d7d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "570d0898145046399920eaf09717204d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "61f6a729d42541a5b89a13f4de816a44": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3da5fcaeb2ee4a5ea83657501f8bfe6a",
       "max": 296,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_44a7c810214740ea9e66b5a55dea2ea4",
       "tabbable": null,
       "tooltip": null,
       "value": 296
      }
     },
     "65b585ddbb3a4956a7eb59aa81d2f954": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c50f9b0131f14f4f8d430c32b3d712e5",
       "placeholder": "​",
       "style": "IPY_MODEL_0f46ec9d328742f6ae76dedf85c852f9",
       "tabbable": null,
       "tooltip": null,
       "value": "special_tokens_map.json: 100%"
      }
     },
     "6c1a1296a61d49ed96b25d8a98848e6b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "8871175aa12144ba9bd273a638fc365d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "90dcf0651aa94a94bd7306bdf794141b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "91ad926008584b139e56dbe7efd10fbe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a2965cf30eec47d58bad0dc349930f6a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_201a3ed0208742648a1d406de51bcb1b",
        "IPY_MODEL_16bb808a8a6c4bed8ccc0b111d2d602a",
        "IPY_MODEL_3cbcbfc46e2340f6bcde6b39f9c9f71c"
       ],
       "layout": "IPY_MODEL_10407867550e4c53a6e7a7e1feef1caf",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a2d71dd9596d46fa9074481ccb2067bd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a74fedcdb5484577b54e69069d0afcb0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b1a89fb65e854b7b921fc52320fe951f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fa25a22acd2e4ac9866587bb7805dc67",
       "placeholder": "​",
       "style": "IPY_MODEL_6c1a1296a61d49ed96b25d8a98848e6b",
       "tabbable": null,
       "tooltip": null,
       "value": " 50.9k/50.9k [00:00&lt;00:00, 3.62MB/s]"
      }
     },
     "b45fc9e726354dd5997cfd4750d7d474": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_65b585ddbb3a4956a7eb59aa81d2f954",
        "IPY_MODEL_61f6a729d42541a5b89a13f4de816a44",
        "IPY_MODEL_ea0d376e0ffd40a7998381ec7004665d"
       ],
       "layout": "IPY_MODEL_8871175aa12144ba9bd273a638fc365d",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b461f09b4dda4625a30f52ed8ab765ef": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8baa62dd5b74ea49a3a2ff365341e6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "baa0f2a6b0704f76bbe94b358e099cb2": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_91ad926008584b139e56dbe7efd10fbe",
       "max": 50870,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_4d19635a3acd46e8914ce45db548d7d6",
       "tabbable": null,
       "tooltip": null,
       "value": 50870
      }
     },
     "c50f9b0131f14f4f8d430c32b3d712e5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cac9fd0263b1462381b99a50c7ade864": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ce860d337f494e6f85c9de10617de011": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d63fc26476d5429e826b288764662dd2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea0d376e0ffd40a7998381ec7004665d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a74fedcdb5484577b54e69069d0afcb0",
       "placeholder": "​",
       "style": "IPY_MODEL_90dcf0651aa94a94bd7306bdf794141b",
       "tabbable": null,
       "tooltip": null,
       "value": " 296/296 [00:00&lt;00:00, 26.6kB/s]"
      }
     },
     "fa25a22acd2e4ac9866587bb7805dc67": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
